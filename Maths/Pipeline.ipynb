{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93fc3d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np \n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.datasets import load_breast_cancer, load_iris, make_blobs\n",
    "from sklearn.model_selection import KFold, LeaveOneOut, ShuffleSplit, GroupKFold, StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81290e53",
   "metadata": {},
   "source": [
    "# Cross validation\n",
    "\n",
    "Il existe plusieurs variantes de la VC dans Sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5c3d7a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "clf  = LogisticRegression()\n",
    "iris.target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0cb9072a",
   "metadata": {},
   "source": [
    "### 1. Naive validation\n",
    "- Diviser simplement l'ensemble de données en train et test.\n",
    "- Division aléatoire des données. \n",
    "- L'exactitude peut s'averer irréalistes si les examples difficiles ou faciles ne sont pas bien répartis.\n",
    "- La division est problématique lorsque la taille du test est petite. \n",
    "- Un petit ensemble de test implique une incertitude statistique autour de l’erreur moyenne estimée, il devient alors difficile de prétendre que l’algorithme A fonctionne mieux que l’algorithme B sur la tâche C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86786a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=41)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3b6fa4",
   "metadata": {},
   "source": [
    "### 2.\tCross validation standard\n",
    "- Très utiliser pour la régression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cf8c574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.98, 0.96, 0.98]), 0.9733333333333333)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By default, cv=3\n",
    "scores = cross_val_score(clf, iris.data, iris.target, cv=3)\n",
    "scores, scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ffd929",
   "metadata": {},
   "source": [
    "Ajuster le nombre de plis utilisé par _cross_val_score_ en utilisant le paramètre _cv_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c561158",
   "metadata": {},
   "source": [
    "### 3. Cross validation non stratifiée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e254ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0.]), 0.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.1 Without shuffle\n",
    "\n",
    "# Chaque pli correspond à une des classes du jeu de donnée (il n'est pas possible d'apprendre quoi que ce soit)\n",
    "kfold = KFold(n_splits=3)\n",
    "scores = cross_val_score(clf, iris.data, iris.target, cv=kfold)\n",
    "scores, scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adc9431c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.  , 0.92, 0.98]), 0.9666666666666667)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.2 With shuffle\n",
    "\n",
    "# Mélanger les échantillons\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(clf, iris.data, iris.target, cv=kfold)\n",
    "scores, scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05a91cb",
   "metadata": {},
   "source": [
    "## 4. Validation croisée leave-one-out\n",
    "- Pour chaque division, un seul point de donnée est choisi pour construire le jeu de test.\n",
    "- Processus tres coûteaux pour de grand jeu de données.\n",
    "- Bonne pour les données de petite taille."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1829662f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset.size : 150\n",
      "Number of CV iterations : 150\n",
      "Mean accuracy : 0.97\n"
     ]
    }
   ],
   "source": [
    "loo = LeaveOneOut()\n",
    "scores = cross_val_score(clf, iris.data, iris.target, cv=loo)\n",
    "print(f\"Dataset.size : {len(iris.data)}\")\n",
    "print(f\"Number of CV iterations : {len(scores)}\")\n",
    "print(f\"Mean accuracy : {scores.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764e93e2",
   "metadata": {},
   "source": [
    "### 5. Validation croisée shuffle-split (repeated)\n",
    "\n",
    "- On échantillonne de manière aléatoire _train_size_ de données pour le trainset et _test_size_ pour le testset, _nsplit_ fois.\n",
    "- Cette méthode permet de contrôler le nombre d'itération indépendamment de la taille des jeux d'apprentissage et de test.\n",
    "- Permet d'utiliser une partie des données lors de chaque itération (ne couvre pas la totalité des échantillons).\n",
    "- Utile pour des jeux de données de grandes tailles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7903c85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation score : [0.93333333 0.94666667 0.96       0.96       0.96       0.90666667\n",
      " 0.97333333 0.94666667 0.94666667 0.98666667]\n",
      "Mean accuracy : 0.95\n"
     ]
    }
   ],
   "source": [
    "shuffle_split = ShuffleSplit(test_size=.5, train_size=.5, n_splits=10)\n",
    "scores = cross_val_score(clf, iris.data, iris.target, cv=shuffle_split)\n",
    "print(f\"Cross validation score : {scores}\")\n",
    "print(f\"Mean accuracy : {scores.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf4bc43",
   "metadata": {},
   "source": [
    "### 6.Validation croisée avec group\n",
    "\n",
    "\n",
    "- Les groupes de données partagent les mêmes traits de similarités.\n",
    "- Un groupe contient les données qui ne devraient pas être séparées lors de la création des jeux de d’apprentissage et test. Exemple:\n",
    "    + Toutes les expressions faciales d’une même personne\n",
    "    + Echantillons du même patient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf802b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75      , 0.6       , 0.66666667])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_blobs(n_samples=12, random_state=0)\n",
    "groups = [0, 0, 0, 1, 1, 1, 1, 2, 2, 3, 3, 3]\n",
    "cross_val_score(clf, X, y, groups=groups, cv=GroupKFold(n_splits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d29916b",
   "metadata": {},
   "source": [
    "### 6.Validation croisée stratifié\n",
    "\n",
    "- Très utiliser pour la classification\n",
    "- Equilibre des classes déséquilibrées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d53d1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split:  1 - Len(X_train): 112 - Len(y_test): 38\n",
      "Classe: 0 - 37 - 13\n",
      "Classe: 1 - 38 - 12\n",
      "Classe: 2 - 37 - 13\n",
      "\n",
      "Split:  2 - Len(X_train): 112 - Len(y_test): 38\n",
      "Classe: 0 - 37 - 13\n",
      "Classe: 1 - 38 - 12\n",
      "Classe: 2 - 37 - 13\n",
      "\n",
      "Split:  3 - Len(X_train): 113 - Len(y_test): 37\n",
      "Classe: 0 - 38 - 12\n",
      "Classe: 1 - 37 - 13\n",
      "Classe: 2 - 38 - 12\n",
      "\n",
      "Split:  4 - Len(X_train): 113 - Len(y_test): 37\n",
      "Classe: 0 - 38 - 12\n",
      "Classe: 1 - 37 - 13\n",
      "Classe: 2 - 38 - 12\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=4, random_state=0, shuffle=True)\n",
    "n_splits = 1\n",
    "for train_ix, test_ix in skf.split(iris.data, iris.target):\n",
    "    print(f\"\\nSplit:  {n_splits} - Len(X_train): {len(train_ix)} - Len(y_test): {len(test_ix)}\")\n",
    "    \n",
    "    # Select rows\n",
    "    train_X, test_X = iris.data[train_ix],   iris.data[test_ix]\n",
    "    train_y, test_y = iris.target[train_ix], iris.target[test_ix]\n",
    "\n",
    "    # Class distribution for the train and test sets for each split\n",
    "    for target in list(np.unique(iris.target)):\n",
    "        print(f\"Classe: {target} - {len(train_y[train_y==target])} - {len(test_y[test_y==target])}\")\n",
    "    n_splits += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff99dda",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "Outil généraliste servant à chaîner et encapsuler plusieurs étapes de traitement dans un flux d'apprentissage automatique dans un seul object Python.\n",
    "\n",
    "Outil très utile pour éviter la fuite d'information lors de la validation croisée et la sélection de paramètres par recherche de grille.\n",
    "\n",
    "Pipeline permet d'écrire du code de manière plus succincte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587c1806",
   "metadata": {},
   "source": [
    "## Naive example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2115139e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.98357\n",
      "Test Score:  0.97902\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=42)\n",
    "\n",
    "# Normalize train data\n",
    "scaler         = MinMaxScaler().fit(X_train)\n",
    "X_train_scaler = scaler.transform(X_train)\n",
    "\n",
    "# Normalize test data\n",
    "X_test_scaler = scaler.transform(X_test)\n",
    "\n",
    "# Model\n",
    "svm = SVC()\n",
    "\n",
    "# Fit the model\n",
    "svm.fit(X_train_scaler, y_train)\n",
    "\n",
    "# Scoring\n",
    "print(f\"Train Score: {svm.score(X_train_scaler, y_train):.5f}\")\n",
    "print(f\"Test Score:  {svm.score(X_test_scaler, y_test):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56445b91",
   "metadata": {},
   "source": [
    "### Fuite de données\n",
    "\n",
    "```Python\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=5)\n",
    "grid.fit(X_train_scaler, y_train)\n",
    "```\n",
    "Nous nous sommes servis de tous les données d'apprentissage recalibrées pour la recherche sur grille.\n",
    "\n",
    "Pour chaque division dans la validation croisée, une certaines partie du jeu de données d'apprentissage original va être déclarée comme étant TRAIN set et une autre comme étant TEST set de cette division.\n",
    "\n",
    "Dans ce cas $\\Rightarrow$ Fuite d'information\n",
    "\n",
    "Pour éviter ce problème, la division du jeu de données au cours de la validation croisée devrait être effectuée avant tout prétraitement $\\Rightarrow$ Utiliser un pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e6dbb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross validation accuracy : 0.9788508891928865\n",
      "Best param : {'C': 1, 'gamma': 1}\n",
      "Test Accuracy score : 0.9790209790209791\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10], 'gamma': [0.001, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=5)\n",
    "# <!> Big mistake \n",
    "grid.fit(X_train_scaler, y_train)\n",
    "print(f\"Best cross validation accuracy : {grid.best_score_}\")\n",
    "print(f\"Best param : {grid.best_params_}\")\n",
    "print(f\"Test Accuracy score : {grid.score(X_test_scaler, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ada8817",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "Pipeline pour exprimer le flux de travail d'apprentissage.\n",
    "\n",
    "L'object Pipeline prend une liste d'étape et chaque étape est un tuple.\n",
    "\n",
    "La classe _Pipeline_ n'est pas restreinte au prétraitement et à la classification.\n",
    "\n",
    "Les conditions à respecter pour les estimateurs d'un pipeline sont:\n",
    "\n",
    "- Toutes les étapes (sauf la dernière) doivent comprendre les méthodes _fit_ et _transform_:\n",
    "    - _transform_ pour qu'ils soient capables de produire une nouvelle représentation des données qui pourra être utilisée dans l'étape suivante\n",
    "- La derniere étape doit comprendre de la fonction _fit_ mais pas forcement de la fonction _predict_\n",
    "\n",
    "Au cours de l'appel à Pipeline.fit, le pipeline appelle _fit_ puis _transform_ ou simplement _fit_transform_ sur chaque étape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f6e5e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 0.97902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('scaler', MinMaxScaler()), ('svm', SVC())]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create pipeline\n",
    "pipe = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC())])\n",
    "\n",
    "# Fit pipeline\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Score \n",
    "print(f\"Test Score: {pipe.score(X_test, y_test):.5f}\")\n",
    "\n",
    "# Steps\n",
    "pipe.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b2bd31",
   "metadata": {},
   "source": [
    "## Accéder aux attributs des étapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "befcab2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scaler': MinMaxScaler(), 'svm': SVC()}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d68c675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.named_steps['svm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fcb04b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.named_steps['svm'].C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae2716",
   "metadata": {},
   "source": [
    "## Pipeline dans des recherches sur grill\n",
    "\n",
    "La syntaxe à employer pour définir une grille dans un pipeline consiste à spécifier :\n",
    "\n",
    "Pour chaque paramètre **nom de l'étape __ non du paramètre exact concerné**.\n",
    "\n",
    "_MinMaxScaler_ est réajusté uniquement pour les données d'apprentissage et donc aucune information ne fuite du jeu de test.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a6e06a",
   "metadata": {},
   "source": [
    "## Method 1 : Pipeline (long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f33294a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross validation accuracy: 0.96944\n",
      "Test set score: 0.97902\n",
      "Best param validation accuracy: {'svm__C': 1, 'svm__gamma': 1}\n",
      "Best estimator:\n",
      "Pipeline(steps=[('scaler', MinMaxScaler()), ('svm', SVC(C=1, gamma=1))])\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'svm__C': [0.001, 0.01, 0.1, 1],\n",
    "              'svm__gamma': [0.001, 0.01, 0.1, 1]}\n",
    "\n",
    "pipe = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC())])\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best cross validation accuracy: {grid.best_score_:.5f}\")\n",
    "print(f\"Test set score: {grid.score(X_test, y_test):.5f}\")\n",
    "print(f\"Best param validation accuracy: {grid.best_params_}\")\n",
    "# Dans notre cas, grid.best_estimator_ est un pipeline avec 2 étapes\n",
    "print(f\"Best estimator:\\n{grid.best_estimator_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4db93e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, gamma=1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_.named_steps[\"svm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df5a71f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_.named_steps[\"svm\"].C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e128c7e",
   "metadata": {},
   "source": [
    "## Method 2 : Make_pipeline (short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22df690c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .... (step 1 of 3) Processing minmaxscaler-1, total=   0.0s\n",
      "[Pipeline] ............... (step 2 of 3) Processing pca, total=   0.0s\n",
      "[Pipeline] .... (step 3 of 3) Processing minmaxscaler-2, total=   0.0s\n",
      "Best cross validation accuracy: 0.96944\n",
      "Test set score: 0.97902\n",
      "Best param validation accuracy: {'svm__C': 1, 'svm__gamma': 1}\n",
      "Best estimator:\n",
      "Pipeline(steps=[('scaler', MinMaxScaler()), ('svm', SVC(C=1, gamma=1))])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('minmaxscaler-1', MinMaxScaler()),\n",
       " ('pca', PCA(n_components=2)),\n",
       " ('minmaxscaler-2', MinMaxScaler())]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MinMaxScaler ont été renommée, mais il est préférable d'utiliser la construction Pipeline \n",
    "# avec des noms explicites afin de rendre chaque étape plus explicite\n",
    "pipe = make_pipeline(MinMaxScaler(), PCA(n_components=2), MinMaxScaler(), verbose=True)\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best cross validation accuracy: {grid.best_score_:.5f}\")\n",
    "print(f\"Test set score: {grid.score(X_test, y_test):.5f}\")\n",
    "print(f\"Best param validation accuracy: {grid.best_params_}\")\n",
    "# Dans notre cas, grid.best_estimator_ est un pipeline avec 2 étapes\n",
    "print(f\"Best estimator:\\n{grid.best_estimator_}\")\n",
    "# Steps\n",
    "pipe.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80103dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross validation accuracy: 0.70815\n",
      "Test set score: 0.75317\n",
      "Best param validation accuracy: {'ridge__alpha': 0.1}\n",
      "Best estimator:\n",
      "Pipeline(steps=[('minmaxscaler', MinMaxScaler()), ('ridge', Ridge(alpha=0.1))])\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'ridge__alpha': [0.001, 0.01, 0.1, 1]}\n",
    "\n",
    "pipe = make_pipeline(MinMaxScaler(), Ridge(), verbose=False)\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best cross validation accuracy: {grid.best_score_:.5f}\")\n",
    "print(f\"Test set score: {grid.score(X_test, y_test):.5f}\")\n",
    "print(f\"Best param validation accuracy: {grid.best_params_}\")\n",
    "print(f\"Best estimator:\\n{grid.best_estimator_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a64413",
   "metadata": {},
   "source": [
    "# Effectuer une recherche sur grill pour trouver quel modèle utiliser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c26530ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('preprocessing', StandardScaler()),\n",
       "                                       ('classifier', SVC())]),\n",
       "             param_grid=[{'classifier': [SVC(gamma=0.01)],\n",
       "                          'classifier__C': [0.001, 0.01, 0.1, 1.0, 1.0],\n",
       "                          'classifier__gamma': [0.001, 0.01, 0.1, 1.0, 1.0],\n",
       "                          'preprocessing': [StandardScaler(), None]},\n",
       "                         {'classifier': [RandomForestClassifier()],\n",
       "                          'classifier__max_features': [1, 2, 3],\n",
       "                          'classifier__n_estimators': [100],\n",
       "                          'preprocessing': [None]}])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = [\n",
    "    # Model 1\n",
    "    {'classifier': [SVC()], 'preprocessing': [StandardScaler(), None],\n",
    "    'classifier__C': [.001, .01, .1, 1., 1.],\n",
    "    'classifier__gamma': [.001, .01, .1, 1., 1.]},\n",
    "    # Model 2\n",
    "    {'classifier': [RandomForestClassifier()], 'preprocessing': [None],\n",
    "    'classifier__n_estimators': [100],\n",
    "    'classifier__max_features': [1, 2, 3]}]\n",
    "\n",
    "# Load dataset\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=42)\n",
    "\n",
    "pipe = Pipeline([('preprocessing', StandardScaler()), ('classifier', SVC())])\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=5)\n",
    "\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a6086d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross validation accuracy: 0.96241\n",
      "Test set score: 0.97902\n",
      "\n",
      "Best param validation accuracy:\n",
      "{'classifier': SVC(gamma=0.01), 'classifier__C': 1.0, 'classifier__gamma': 0.01, 'preprocessing': StandardScaler()}\n",
      "\n",
      "Best estimator:\n",
      "Pipeline(steps=[('preprocessing', StandardScaler()),\n",
      "                ('classifier', SVC(gamma=0.01))])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best cross validation accuracy: {grid.best_score_:.5f}\")\n",
    "print(f\"Test set score: {grid.score(X_test, y_test):.5f}\\n\")\n",
    "print(f\"Best param validation accuracy:\\n{grid.best_params_}\\n\")\n",
    "print(f\"Best estimator:\\n{grid.best_estimator_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
